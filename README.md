# ğŸ“º NETFLIX ELT DATA PIPELINE

## ğŸ“Œ Overview

This project implements an **end-to-end ELT data pipeline** for Netflix datasets using **AWS S3, Snowflake, and dbt**. The pipeline ingests raw data, applies transformations using dbt, and builds analytics-ready **fact and dimension tables**, including **SCD Type 2 snapshots**.

---

## ğŸ—ï¸ Architecture

**Source â†’ Data Lake â†’ Warehouse â†’ Transform â†’ Analytics**

* **AWS S3 (LocalStack)** â€“ Raw data landing (Data Lake)
* **Snowflake** â€“ Data Warehouse
* **dbt** â€“ Transformations, testing, snapshots, and modeling

---

## ğŸ› ï¸ Tech Stack

* AWS S3 (via LocalStack)
* Snowflake
* dbt (dbt-core, dbt-snowflake)
* Python
* SnowSQL

---

## ğŸ“‚ Data Layers

* **Raw Landing Layer** â€“ Original ingested data
* **Staging Layer** â€“ Copy of raw data for safety
* **Dev Layer** â€“ Transformed data using dbt (facts & dimensions)

---

## ğŸ” dbt Features Used

* Models (staging, fact, dimension)
* Materializations (view, table, incremental, ephemeral)
* Seeds (static reference data)
* Sources (raw data definitions)
* Snapshots (SCD Type 2)
* Tests (generic & singular)
* dbt_utils package

---

## âš™ï¸ Pipeline Flow

1. Upload CSV data to **AWS S3 (LocalStack)**
2. Load data into **Snowflake RAW tables**
3. Transform data using **dbt staging models**
4. Build **fact & dimension tables**
5. Implement **incremental models**
6. Track history using **dbt snapshots (SCD2)**
7. Apply **data quality tests**

---

## ğŸ“Š Key Outcomes

* Analytics-ready star schema
* Incremental fact tables
* Historical tracking using SCD Type 2
* Modular, scalable ELT design

---

## ğŸ“š References

* Netflix Dataset
* dbt Documentation
* Snowflake Documentation

---

âœ… This project demonstrates **modern data engineering best practices** using ELT, dbt modeling, testing, and snapshots.

In progress......
---
